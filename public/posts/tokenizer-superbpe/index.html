<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE | George Grigorev Blog</title>
<meta name="keywords" content="tokenization, bpe, llm">
<meta name="description" content="Byte-level BPE from first principles: what matters for speed and quality, how to implement it cleanly, and why a SuperBPE variant can lift sample efficiency.">
<meta name="author" content="George Grigorev">
<link rel="canonical" href="https://ggrigorev.me/posts/tokenizer-superbpe/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ggrigorev.me/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ggrigorev.me/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ggrigorev.me/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ggrigorev.me/apple-touch-icon.png">
<link rel="mask-icon" href="https://ggrigorev.me/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ggrigorev.me/posts/tokenizer-superbpe/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="stylesheet" href="/css/custom.css"><script>
    window.MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
        processEscapes: true,
        tags: 'ams',
        packages: { '[+]': ['noerrors'] },
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      },
      loader: { load: ['[tex]/noerrors'] },
    };
  </script>
  <script
    async
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
  ></script>
  <script>
    window.addEventListener('DOMContentLoaded', function () {
      if (window.MathJax && window.MathJax.typesetPromise) {
        window.MathJax.typesetPromise();
      }
    });
  </script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-F5HXWCK8S4"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-F5HXWCK8S4');
        }
      </script><meta property="og:url" content="https://ggrigorev.me/posts/tokenizer-superbpe/">
  <meta property="og:site_name" content="George Grigorev Blog">
  <meta property="og:title" content="Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE">
  <meta property="og:description" content="Byte-level BPE from first principles: what matters for speed and quality, how to implement it cleanly, and why a SuperBPE variant can lift sample efficiency.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:tag" content="Tokenization">
    <meta property="article:tag" content="Bpe">
    <meta property="article:tag" content="Llm">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE">
<meta name="twitter:description" content="Byte-level BPE from first principles: what matters for speed and quality, how to implement it cleanly, and why a SuperBPE variant can lift sample efficiency.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://ggrigorev.me/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE",
      "item": "https://ggrigorev.me/posts/tokenizer-superbpe/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE",
  "name": "Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE",
  "description": "Byte-level BPE from first principles: what matters for speed and quality, how to implement it cleanly, and why a SuperBPE variant can lift sample efficiency.",
  "keywords": [
    "tokenization", "bpe", "llm"
  ],
  "articleBody": "In this blog post I will share my experience with building a BPE tokenizer from scratch, analyzing its quality, sample efficiency, and efficient implementation.\nThis work started as part of the excellent CS336 “LLMs from Scratch” course, but then quickly developed into its own mini-project due to my curiosity to explore things further.\nI’ve now shared all code in my repo\nIntro Tokenization is needed to convert your text into numeric representation that is useful for training LLMs (and to essentially compress your data too, reducing sequence size)\nLet’s discuss BPE tokenization algorithm:\nGeneral scheme of BPE training\nEach string character in UTF-8 encoding is represented as bytes (values from 0 to 255). We convert text to bytes. This is our initial set of tokens.\nAt each step: 1.\tCount adjacent token pair frequencies. 2.\tMerge the most frequent pair into a new token ID (sequence becomes shorter). 3.\tRepeat until you hit your target vocab size.\nLarger token ID -\u003e later it was created from merge.\nByte-level BPE accepts bytes and returns sequence of ints (this can be saved as *uint16* if your vocab size is less than 65536), and no input is left un-tokenized (in contrast to earlier tokenizers which used token for unknown tokens that don’t fit into vocab)\nSingle training step of BPE algorithm\nHere we make a frequency count for each pair of numbers, select max pair by this count and replace it with a new token ID.\nThere are few nuances in the implementation.\nNot all bytes are created equal Since byte is just a number from 0 to 255, but we have much more characters than 256, we represent it with variable amount of bytes. An average English text would be 1.25 bytes/character.\nEach UTF-8 code point is represented by 1–4 bytes, depending on the character. ASCII (U+0000–U+007F) → 1 byte Latin, Cyrillic, etc. → 2 bytes Most CJK (Chinese/Japanese/Korean) → 3 bytes Rare supplementary (emoji, historic scripts) → 4 bytes Pre-tokenization It’s easy to show the naïve algorithm is $O(N^2)$\nFor large files, even a single iteration becomes very computationally expensive (for 5 GB file, $N$ would be 4e9 and single step would take 100+ days on modern CPUs!)\nOne optimization that we could use is to split whole training corpus into words (split by token ID 32 (\" “) in this example – or, more generally, by any whitespace character) and group words together via hashmap.\nThen, knowing the count of each pre-token, true count for a pair of numbers becomes frequency(pair within word) × pre-token count\nIt’s even better to split not just by delimiter, but by a more complicated regex. For example, this is the regex used when training GPT-2:\n'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+ It splits text “Let’s consider tokenization word-by-word” like this:\n['Let', \"'s\", ' consider', ' tokenization', ' word', '-', 'by', '-', 'word'] Notice we attach whitespace to the left.\nWhat’s SOTA in regex patterns? Nowadays, people use better regex to handle code, long numbers or delimiters smarter.\nDigits: triad chunking with \\p{N}{1,3}; support for thousands separators (, and _), decimals, scientific notation, and percents. These help BPE learn reusable numeric chunks without exploding vocab. It also improves math benchmarks since we represent numbers more faithfully. Contractions: handle both ASCII ’ and curly ’, case-insensitive, so “It’s”, “IT’S”, “it’s” → consistent splits. Punctuation: keep [^\\s\\p{L}\\p{N}]+ to group operators/emoji/punct as learned units. (this part is advanced and requires a non-regex grapheme/emoji segmenter; regex alone won’t cleanly capture all emoji families.) In addition, when it comes to data preprocessing, there’s also a notion of special tokens that we do not split into subwords, or into bytes, i.e., they are not involved in the training. One of the most popular special token examples - end of text. It is important to keep it as it is as we use it to split documents with each other, and tell the LLM where the sequence ends, so that at inference we would have an LLM that can actually stop its generation :)\nWrapping up Generally, now whole training pipeline looks like Pretokenization + N iterations of merges.\nPretokenization was rewritten in Rust (via PyO3). The Rust code splits text with a regex and counts pre-tokens directly, avoiding round-tripping large Python string lists (pickle/marshalling). I used simdutf, ahash for efficient decoding and counting.\nSingle-process benchmark (1.5M lines):\nBaseline (Python): 67.2 s total – split: 45.6 s, count: 17.3 s Iter 1 (Rust split, Python count): ~26.0 s (split) + 17.3 s (count) Iter 2 (Rust split + count): 21.2 s total Code\nOther details There are still a couple of questions related to merging process:\nHow to detect the most frequent pair inside a pre-token efficiently? Do we need some specific data structure? How to most efficiently merge two tokens together and update the list in-place? Is there a difference between encoding during training and during inference? Let me answer all of those questions at once. I will share the data structures, some hacks, and the inference implementation. That might not be the best algorithm overall, but this is what I found worked for me and hopefully this can give you some food for thought.\nFirst of all, for training our main goal is to be able to find the most frequent pair very quickly. Unfortunately there’s not much we could do to merge faster during training. We still need to linearly scan the sequence.\nLet’s cache pre_token_counts, updated_pre_tokens, all_pair_counts, pair_to_pre_tokens.\nall_pair_counts is the most important map. We take the top-1 count each step to get the max pair.\nTwo observations:\nAfter each update, we don’t need to re-compute all_pair_counts. We know that most pairs are untouched, so we keep only pre-tokens that have been updated (updated_pre_tokens). Moreover, we know that frequencies of pairs change in deterministic order. So, if we had a sequence (1, 2, 3, 4) and merged (2, 3) -\u003e 99: Subtract counts for pairs destroyed by the merge. Adjust counts for pairs that touch the merged span (e.g., (1,2)→(1,99), (3,4)→(99,4)). Leave everything else alone. Including these modifications lets us cache all_pair_counts and update it after each iteration instead of re-computing.\nThis change reduced my total training time 7x.\nWe know there are situations where top-k pairs from all_pair_counts remain the same (so top-2 becomes top-1 in the next step, and if we know that in advance, we could retrieve next max pair in O(1)). Intuitively, this can be if our updated frequencies in all_pair_counts do not touch any of the top-k keys, or at least do not drive them too high. This is a pretty weak assumption but it helps to reduce the search space and avoid resorting keys of all_pair_counts every iteration. A natural data structure for all_pair_counts is a max-heap. Updates are $O(log(N))$, and retrieving the max is $O(1)$. When I tried it, I noticed that we add/remove keys too frequently, so that the overhead of max-heap becomes too high. I keep it as a dict and try to sort elements when needed.\nSo in practice I have heuristics to check if top-k keys in all pairs counts are not updated, so I reduce total amounts of sorting to find the most frequent pair. I suspect we could go even further and employ probabilistic algorithms.\nInference Now we don’t need to find the most frequent token anymore, and all of our merges are static. We convert text to bytes and apply all merges in the same order as we obtained them during training. What’s the best data structure to store and apply the merges to our sequence?\nI ended up using doubly linked list for the sequence of tokens for each word (Note that in this case we don’t strictly need pre-tokens; we can process the whole sequence until we’ve applied all merges)\nWe start with a chain of tokens, each with pointers to the next and previous tokens. We do a linear scan until we find the corresponding merge, replace the first element with the merged token ID, unlink second element, re-link updated token to the next.next. Finally, LeetCode is useful!!!\nSuperBPE After we trained tokenizer and spent so much time implementing it from scratch instead of just using huggingface tokenizers, we need to justify all of that time and train something more useful. Welcome SuperBPE.\nSuperBPE is a fairly recent work, and in my opinion underexplored. When I sat implementing BPE from scratch I couldn’t get rid of an idea of this pre-tokenization. Why restrict ourselves to single words? Doing pre-tokens seems like a useful optimization hack, but language might not always work in words, but rather concepts? I could easily imagine cases where frequencies of multi-word spans can exceed those of adjacent byte-pairs. Language is redundant and contains phrases like “such as”, “to be considered”, “for example” which is logical to represent as a single token. SuperBPE solves exactly that. Now tokens can represent multiple words at once.\nIn order to implement SuperBPE, the only thing we need to change is our “pre_token_counts” representation. Instead of words going to each key, we could put a whole document there. It’s a huge dict, but the algorithm still works.\nI tested it and consistently hit OOM, or my training would never finish (actually it’s the same problem as we started this blog post with). So I added a few heuristics: I split on punctuation; cap pre-token length at 10 tokens (only when a span exceeds 20 tokens), and apply few other heuristics + reduced dataset size.\nOverview of SuperBPE\nFinal algorithm is still like 20x slower than my BPE implementation, but hopefully we don’t need to start from the beginning, as we could enable SuperBPE only at the last 20% of the training (this works better in practice, since we want for the BPE to learn general concepts inside words first (inter-word n-gram dependencies), and then lift this restriction)\nPractical aspects Okay, imagine you now have your implementation of tokenizer that is so flexible it allows your custom way of training. But a) no one cares if it’s not supported in tokenizers b) it’s really slow (it will require full rust rewrite and still for very large datasets will be slow)\nSo converting to tokenizers format to the rescue, turns out it’s not that hard!\nAfter inspecting some tokenizer vocabs (it’s stored inside tokenizer.json) i noticed a few things: a) it’s json, but my vocab is in bytes. I tried converting bytes to str, and that’s the whole point of Byte Level BPE - you cannot convert it to UTF-8 easily 😄 HF uses some interesting choice of mapping bytes to some obscure UTF-8 characters. Whitespace becomes Ġ , \\n becomes\nExperiments The most interesting part.\nI first trained my BPE implementation on good quality data. (About 500M tokens or 5 GB text file). I used an improved regex template with digit grouping and better splitting.\nFinished training in 38,562.8 s. Average iteration time: 0.764 s. Total sort time was 17,350.43 s.\nIt achieved compression ratio: 4.39 bytes per token\nTokenization speed on a full 12 GB file: 1193.2s -\u003e 10.3 MB/s\nSuperBPE is trained by resuming the basic BPE run from 26,000th iteration (roughly 80% of training).\nFinished training in 53 hours Sort time is 49% of it\n!!! It’s 20x slower, let’s see if it’s worth it\nI have used my baseline implementation of LLM training with hand-written Triton FlashAttention-2 and some tweaks.\nJust trained a 70M-param LLM to \u003c20 perplexity on DCLM in 5 hours – on a single consumer GPU.\nAll my convergence + sample-efficiency tricks are stacking beautifully.\nI now have separate repo with my research related to sample efficient GPT training, link in reply.\n— George Grigorev (@iamgrigorev) October 5, 2025 LLM Training Two tokenizers trained with 32700 vocab size: “baseline BPE tokenizer” and “SuperBPE” tokenizer\nDataset used: DCLM (edu subset, filtered with score \u003e 2.75) Token count: ~1.2B (24k iterations, 512 context length, 96 batch size, single GPU – RTX5090) Model vocab size: 32768 (multiple of 16 for better GPU utilization)\nNon-embedding parameter count: 70M\nAfter SuperBPE, dataset has 20% fewer tokens, so training is 20% more sample-efficient!\nThis matches results from the paper authors:\nSuper excited to see @moondream_ai's newest model use SuperBPE!! We did a little bit of analysis — using SuperBPE reduced their seqlen by 21% on average and made the token frequency distribution more uniform, meaning fewer hyper-frequent \u0026 hyper-rare tokens! https://t.co/6ZWNYWPOyx pic.twitter.com/KIk3gL5lvs\n— Alisa Liu @ COLM 🦙 (@alisawuffles) September 29, 2025 Let’s now analyze tokens that are produced in SuperBPE:\nI run training using optimal settings with Muon optimizer for 2D params and AdamW for 1D params and here’s my loss results for training:\nFirst run\nAfter running first experiment, I noticed significantly higher loss. Then I checked outputs, it looked great, subjectively even better than the baseline. Something is wrong! What’s suspicious is that val loss is exactly 25% higher (3.19 -\u003e 3.98). A-ha! Since each token now covers ~25% more text (because 1/(1-0.2)=1.25) and the model’s per-byte compression didn’t change, the per-token NLL (negative log-likelihood) should rise by that same 1.25x factor.\nI calculated train/val loss multipliers by comparing dataset lengths in tokens and re-run experiments.\nLoss scaled run\nInteresting, we are seeing identical plots. However, SuperBPE shows slightly lower grad-norm and pre-activation norms!\nPre-activation norms\nConsidering that during generation your model should now produce 20% less tokens, validation loss is the same as the baseline, and lower norms, we could call SuperBPE tokens bring more regularization, and consider this method a free lunch!\n",
  "wordCount" : "2254",
  "inLanguage": "en",
  "datePublished": "2025-10-05T00:00:00Z",
  "dateModified": "2025-10-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "George Grigorev"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ggrigorev.me/posts/tokenizer-superbpe/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "George Grigorev Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ggrigorev.me/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ggrigorev.me/" accesskey="h" title="George Grigorev Blog (Alt + H)">George Grigorev Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ggrigorev.me/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://ggrigorev.me/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://ggrigorev.me/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Lessons from Building a Tokenizer from Scratch — and a Peek at SuperBPE
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-10-05 00:00:00 +0000 UTC'>October 5, 2025</span>&nbsp;·&nbsp;<span>11 min</span>&nbsp;·&nbsp;<span>2254 words</span>&nbsp;·&nbsp;<span>George Grigorev</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro" aria-label="Intro">Intro</a><ul>
                        
                <li>
                    <a href="#not-all-bytes-are-created-equal" aria-label="Not all bytes are created equal">Not all bytes are created equal</a></li></ul>
                </li>
                <li>
                    <a href="#pre-tokenization" aria-label="Pre-tokenization">Pre-tokenization</a><ul>
                        
                <li>
                    <a href="#whats-sota-in-regex-patterns" aria-label="What’s SOTA in regex patterns?">What’s SOTA in regex patterns?</a></li>
                <li>
                    <a href="#wrapping-up" aria-label="Wrapping up">Wrapping up</a></li></ul>
                </li>
                <li>
                    <a href="#other-details" aria-label="Other details">Other details</a><ul>
                        
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a></li></ul>
                </li>
                <li>
                    <a href="#superbpe" aria-label="SuperBPE">SuperBPE</a><ul>
                        
                <li>
                    <a href="#practical-aspects" aria-label="Practical aspects">Practical aspects</a></li></ul>
                </li>
                <li>
                    <a href="#experiments" aria-label="Experiments">Experiments</a><ul>
                        
                <li>
                    <a href="#llm-training" aria-label="LLM Training">LLM Training</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this blog post I will share my experience with building a BPE tokenizer from scratch, analyzing its quality, sample efficiency, and efficient implementation.</p>
<p>This work started as part of the excellent CS336 “LLMs from Scratch” course, but then quickly developed into its own mini-project due to my curiosity to explore things further.</p>
<p>I&rsquo;ve now shared all code in my <a href="https://github.com/thepowerfuldeez/sample_efficient_gpt">repo</a></p>
<h3 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h3>
<p>Tokenization is needed to convert your text into numeric representation that is useful for training LLMs (and to essentially compress your data too, reducing sequence size)</p>
<p>Let&rsquo;s discuss BPE tokenization algorithm:</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/bpe_scheme.svg"
         alt="General scheme of BPE training"/> <figcaption>
            <p>General scheme of BPE training</p>
        </figcaption>
</figure>

<p>Each string character in UTF-8 encoding is represented as bytes (values from 0 to 255). We convert text to bytes. This is our initial set of tokens.</p>
<p>At each step:
1.	Count adjacent token pair frequencies.
2.	Merge the most frequent pair into a new token ID (sequence becomes shorter).
3.	Repeat until you hit your target vocab size.</p>
<p>Larger token ID -&gt; later it was created from merge.</p>
<p>Byte-level BPE accepts bytes and returns sequence of ints (this can be saved as <code>*uint16*</code> if your vocab size is less than 65536), and no input is left un-tokenized (in contrast to earlier tokenizers which used <code>&lt;UNK&gt;</code> token for unknown tokens that don&rsquo;t fit into vocab)</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/bpe_train_step.svg"
         alt="Single training step of BPE algorithm"/> <figcaption>
            <p>Single training step of BPE algorithm</p>
        </figcaption>
</figure>

<p>Here we make a frequency count for each pair of numbers, select max pair by this count and replace it with a new token ID.</p>
<p>There are few nuances in the implementation.</p>
<h4 id="not-all-bytes-are-created-equal">Not all bytes are created equal<a hidden class="anchor" aria-hidden="true" href="#not-all-bytes-are-created-equal">#</a></h4>
<p>Since byte is just a number from 0 to 255, but we have much more characters than 256, we represent it with variable amount of bytes. An average English text would be 1.25 bytes/character.</p>
<ul>
<li>Each UTF-8 code point is represented by 1–4 bytes, depending on the character.
<ul>
<li>ASCII (U+0000–U+007F) → 1 byte</li>
<li>Latin, Cyrillic, etc. → 2 bytes</li>
<li>Most CJK (Chinese/Japanese/Korean) → 3 bytes</li>
<li>Rare supplementary (emoji, historic scripts) → 4 bytes</li>
</ul>
</li>
</ul>
<h3 id="pre-tokenization">Pre-tokenization<a hidden class="anchor" aria-hidden="true" href="#pre-tokenization">#</a></h3>
<p>It&rsquo;s easy to show the naïve algorithm is $O(N^2)$</p>
<p>For large files, even a single iteration becomes very computationally expensive (for 5 GB file, $N$ would be 4e9 and single step would take 100+ days on modern CPUs!)</p>
<p>One optimization that we could use is to split whole training corpus into words (split by token ID 32 (&quot; &ldquo;) in this example – or, more generally, by any whitespace character) and group words together via hashmap.</p>
<p>Then, knowing the count of each pre-token, true count for a pair of numbers becomes <code>frequency(pair within word) × pre-token count</code></p>
<p>It&rsquo;s even better to split not just by delimiter, but by a more complicated regex. For example, this is the regex used when training GPT-2:</p>
<pre tabindex="0"><code>&#39;(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+
</code></pre><p>It splits text &ldquo;Let&rsquo;s consider tokenization word-by-word&rdquo; like this:</p>
<pre tabindex="0"><code>[&#39;Let&#39;, &#34;&#39;s&#34;, &#39; consider&#39;, &#39; tokenization&#39;, &#39; word&#39;, &#39;-&#39;, &#39;by&#39;, &#39;-&#39;, &#39;word&#39;]
</code></pre><p>Notice we attach whitespace to the left.</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/pretokenization.svg"/> 
</figure>

<h4 id="whats-sota-in-regex-patterns">What’s SOTA in regex patterns?<a hidden class="anchor" aria-hidden="true" href="#whats-sota-in-regex-patterns">#</a></h4>
<p>Nowadays, people use better regex to handle code, long numbers or delimiters smarter.</p>
<ul>
<li><strong>Digits:</strong> triad chunking with <code>\p{N}{1,3}</code>; support for thousands separators (, and _), decimals, scientific notation, and percents. These help BPE learn reusable numeric chunks without exploding vocab. It also improves math benchmarks since we represent numbers more faithfully.</li>
<li><strong>Contractions:</strong> handle both ASCII &rsquo; and curly ’, case-insensitive, so &ldquo;It’s&rdquo;, &ldquo;IT’S&rdquo;, &ldquo;it’s&rdquo; → consistent splits.</li>
<li><strong>Punctuation:</strong> keep <code>[^\s\p{L}\p{N}]+</code> to group operators/emoji/punct as learned units. (this part is advanced and requires a non-regex grapheme/emoji segmenter; regex alone won’t cleanly capture all emoji families.)</li>
</ul>
<p>In addition, when it comes to data preprocessing, there&rsquo;s also a notion of special tokens that we do not split into subwords, or into bytes, i.e., they are not involved in the training. One of the most popular special token examples - end of text. It is important to keep it as it is as we use it to split documents with each other, and tell the LLM where the sequence ends, so that at inference we would have an LLM that can actually stop its generation :)</p>
<h4 id="wrapping-up">Wrapping up<a hidden class="anchor" aria-hidden="true" href="#wrapping-up">#</a></h4>
<p>Generally, now whole training pipeline looks like Pretokenization + N iterations of merges.</p>
<p>Pretokenization was rewritten in Rust (via PyO3). The Rust code splits text with a regex and counts pre-tokens directly, avoiding round-tripping large Python string lists (pickle/marshalling). I used simdutf, ahash for efficient decoding and counting.</p>
<p>Single-process benchmark (1.5M lines):</p>
<ul>
<li>Baseline (Python): 67.2 s total
– split: 45.6 s, count: 17.3 s</li>
<li>Iter 1 (Rust split, Python count): ~26.0 s (split) + 17.3 s (count)</li>
<li>Iter 2 (Rust split + count): 21.2 s total</li>
</ul>
<p><a href="https://github.com/thepowerfuldeez/sample_efficient_gpt/blob/main/sample_efficient_gpt/fastsplit/src/lib.rs">Code</a></p>
<h3 id="other-details">Other details<a hidden class="anchor" aria-hidden="true" href="#other-details">#</a></h3>
<p>There are still a couple of questions related to merging process:</p>
<ol>
<li>How to detect the most frequent pair inside a pre-token efficiently? Do we need some specific data structure?</li>
<li>How to most efficiently merge two tokens together and update the list in-place?</li>
<li>Is there a difference between encoding during training and during inference?</li>
</ol>
<p>Let me answer all of those questions at once. I will share the data structures, some hacks, and the inference implementation. That might not be the best algorithm overall, but this is what I found worked for me and hopefully this can give you some food for thought.</p>
<p>First of all, for training our main goal is to be able to find the most frequent pair very quickly. Unfortunately there&rsquo;s not much we could do to merge faster during training. We still need to linearly scan the sequence.</p>
<p>Let&rsquo;s cache <code>pre_token_counts</code>, <code>updated_pre_tokens</code>, <code>all_pair_counts</code>, <code>pair_to_pre_tokens</code>.</p>
<p><code>all_pair_counts</code> is the most important map. We take the top-1 count each step to get the max pair.</p>
<p>Two observations:</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/all_pair_counts_step.svg"/> 
</figure>

<ol>
<li>After each update, we don&rsquo;t need to re-compute <code>all_pair_counts</code>. We know that most pairs are untouched, so we keep only pre-tokens that have been updated (updated_pre_tokens). Moreover, we know that frequencies of pairs change in deterministic order. So, if we had a sequence (1, 2, 3, 4) and merged (2, 3) -&gt; 99:
<ul>
<li>Subtract counts for pairs destroyed by the merge.</li>
<li>Adjust counts for pairs that touch the merged span (e.g., (1,2)→(1,99), (3,4)→(99,4)).</li>
<li>Leave everything else alone.</li>
</ul>
</li>
</ol>
<p>Including these modifications lets us cache <code>all_pair_counts</code> and update it after each iteration instead of re-computing.</p>
<blockquote>
<p>This change reduced my total training time 7x.</p></blockquote>
<ol start="2">
<li>We know there are situations where top-k pairs from <code>all_pair_counts</code> remain the same (so top-2 becomes top-1 in the next step, and if we know that in advance, we could retrieve next max pair in O(1)). Intuitively, this can be if our updated frequencies in <code>all_pair_counts</code> do not touch any of the top-k keys, or at least do not drive them too high. This is a pretty weak assumption but it helps to reduce the search space and avoid resorting keys of <code>all_pair_counts</code> every iteration.</li>
</ol>
<p>A natural data structure for <code>all_pair_counts</code> is a max-heap. Updates are $O(log(N))$, and retrieving the max is $O(1)$. When I tried it, I noticed that we add/remove keys too frequently, so that the overhead of max-heap becomes too high. I keep it as a dict and try to sort elements when needed.</p>
<p>So in practice I have heuristics to check if top-k keys in all pairs counts are not updated, so I reduce total amounts of sorting to find the most frequent pair. I suspect we could go even further and employ probabilistic algorithms.</p>
<h4 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h4>
<p>Now we don&rsquo;t need to find the most frequent token anymore, and all of our merges are static. We convert text to bytes and apply all merges in the same order as we obtained them during training. What&rsquo;s the best data structure to store and apply the merges to our sequence?</p>
<p>I ended up using doubly linked list for the sequence of tokens for each word (Note that in this case we don’t strictly need pre-tokens; we can process the whole sequence until we’ve applied all merges)</p>
<p>We start with a chain of tokens, each with pointers to the next and previous tokens. We do a linear scan until we find the corresponding merge, replace the first element with the merged token ID, unlink second element, re-link updated token to the next.next. Finally, LeetCode is useful!!!</p>
<h3 id="superbpe">SuperBPE<a hidden class="anchor" aria-hidden="true" href="#superbpe">#</a></h3>
<p>After we trained tokenizer and spent so much time implementing it from scratch instead of just using huggingface tokenizers, we need to justify all of that time and train something more useful. Welcome SuperBPE.</p>
<p>SuperBPE is a fairly recent work, and in my opinion underexplored. When I sat implementing BPE from scratch I couldn&rsquo;t get rid of an idea of this pre-tokenization. Why restrict ourselves to single words? Doing pre-tokens seems like a useful optimization hack, but language might not always work in words, but rather concepts? I could easily imagine cases where frequencies of multi-word spans can exceed those of adjacent byte-pairs. Language is redundant and contains phrases like &ldquo;such as&rdquo;, &ldquo;to be considered&rdquo;, &ldquo;for example&rdquo; which is logical to represent as a single token. SuperBPE solves exactly that. Now tokens can represent multiple words at once.</p>
<p>In order to implement SuperBPE, the only thing we need to change is our &ldquo;pre_token_counts&rdquo; representation. Instead of words going to each key, we could put a whole document there. It’s a huge dict, but the algorithm still works.</p>
<p>I tested it and consistently hit OOM, or my training would never finish (actually it&rsquo;s the same problem as we started this  blog post with). So I added a few heuristics: I split on punctuation; cap pre-token length at 10 tokens (only when a span exceeds 20 tokens), and apply few other heuristics + reduced dataset size.</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/superbpe_heuristics.svg"
         alt="Overview of SuperBPE"/> <figcaption>
            <p>Overview of SuperBPE</p>
        </figcaption>
</figure>

<p>Final algorithm is still like 20x slower than my BPE implementation, but hopefully we don&rsquo;t need to start from the beginning, as we could enable SuperBPE only at the last 20% of the training (this works better in practice, since we want for the BPE to learn general concepts inside words first (inter-word n-gram dependencies), and then lift this restriction)</p>
<h4 id="practical-aspects">Practical aspects<a hidden class="anchor" aria-hidden="true" href="#practical-aspects">#</a></h4>
<p>Okay, imagine you now have your implementation of tokenizer that is so flexible it allows your custom way of training.
But
a) no one cares if it&rsquo;s not supported in tokenizers
b) it&rsquo;s really slow (it will require full rust rewrite and still for very large datasets will be slow)</p>
<p>So converting to tokenizers format to the rescue, turns out it&rsquo;s not that hard!</p>
<p>After inspecting some tokenizer vocabs (it&rsquo;s stored inside tokenizer.json) i noticed a few things:
a) it&rsquo;s json, but my vocab is in bytes.
I tried converting bytes to str, and that&rsquo;s the whole point of Byte Level BPE - you cannot convert it to UTF-8 easily 😄
HF uses some interesting choice of mapping bytes to some obscure UTF-8 characters. Whitespace becomes Ġ , \n becomes</p>
<h3 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h3>
<p>The most interesting part.</p>
<p>I first trained my BPE implementation on good quality data. (About 500M tokens or 5 GB text file). I used an improved regex template with digit grouping and better splitting.</p>
<p>Finished training in 38,562.8 s.
Average iteration time: 0.764 s.
Total sort time was 17,350.43 s.</p>
<p>It achieved compression ratio: 4.39 bytes per token</p>
<p>Tokenization speed on a full 12 GB file: 1193.2s -&gt; 10.3 MB/s</p>
<p>SuperBPE is trained by resuming the basic BPE run from 26,000th iteration (roughly 80% of training).</p>
<p>Finished training in 53 hours
Sort time is 49% of it</p>
<p>!!! It&rsquo;s 20x slower, let&rsquo;s see if it&rsquo;s worth it</p>
<p>I have used my baseline implementation of LLM training with hand-written Triton FlashAttention-2 and some tweaks.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Just trained a 70M-param LLM to &lt;20 perplexity on DCLM in 5 hours – on a single consumer GPU.<br><br>All my convergence + sample-efficiency tricks are stacking beautifully.<br><br>I now have separate repo with my research related to sample efficient GPT training, link in reply.</p>&mdash; George Grigorev (@iamgrigorev) <a href="https://twitter.com/iamgrigorev/status/1974794103021044001?ref_src=twsrc%5Etfw">October 5, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h4 id="llm-training">LLM Training<a hidden class="anchor" aria-hidden="true" href="#llm-training">#</a></h4>
<p>Two tokenizers trained with 32700 vocab size: &ldquo;baseline BPE tokenizer&rdquo; and &ldquo;SuperBPE&rdquo; tokenizer</p>
<p>Dataset used: DCLM (edu subset, filtered with score &gt; 2.75)
Token count: ~1.2B (24k iterations, 512 context length, 96 batch size, single GPU – RTX5090)
Model vocab size: 32768 (multiple of 16 for better GPU utilization)</p>
<p>Non-embedding parameter count: 70M</p>
<p>After SuperBPE, dataset has 20% fewer tokens, so training is 20% more sample-efficient!</p>
<p>This matches results from the paper authors:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Super excited to see <a href="https://twitter.com/moondream_ai?ref_src=twsrc%5Etfw">@moondream_ai</a>&#39;s newest model use SuperBPE!! We did a little bit of analysis — using SuperBPE reduced their seqlen by 21% on average and made the token frequency distribution more uniform, meaning fewer hyper-frequent &amp; hyper-rare tokens! <a href="https://t.co/6ZWNYWPOyx">https://t.co/6ZWNYWPOyx</a> <a href="https://t.co/KIk3gL5lvs">pic.twitter.com/KIk3gL5lvs</a></p>&mdash; Alisa Liu @ COLM 🦙 (@alisawuffles) <a href="https://twitter.com/alisawuffles/status/1972727389634621513?ref_src=twsrc%5Etfw">September 29, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Let&rsquo;s now analyze tokens that are produced in SuperBPE:</p>
<p>I run training using optimal settings with Muon optimizer for 2D params and AdamW for 1D params and here&rsquo;s my loss results for training:</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/superbpe_first_run.jpeg"
         alt="First run"/> <figcaption>
            <p>First run</p>
        </figcaption>
</figure>

<p>After running first experiment, I noticed significantly higher loss. Then I checked outputs, it looked great, subjectively even better than the baseline. Something is wrong!
What&rsquo;s suspicious is that val loss is exactly 25% higher (3.19 -&gt; 3.98). A-ha! Since each token now covers ~25% more text (because 1/(1-0.2)=1.25) and the model’s per-byte compression didn’t change, the per-token NLL (negative log-likelihood) should rise by that same 1.25x factor.</p>
<p>I calculated train/val loss multipliers by comparing dataset lengths in tokens and re-run experiments.</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/superbpe_scaled_run.jpeg"
         alt="Loss scaled run"/> <figcaption>
            <p>Loss scaled run</p>
        </figcaption>
</figure>

<p>Interesting, we are seeing identical plots. However, SuperBPE shows slightly lower grad-norm and pre-activation norms!</p>
<figure>
    <img loading="lazy" src="/posts/tokenizer-superbpe/superbpe_run_prenorms.jpeg"
         alt="Pre-activation norms"/> <figcaption>
            <p>Pre-activation norms</p>
        </figcaption>
</figure>

<p>Considering that during generation your model should now produce 20% less tokens, validation loss is the same as the baseline, and lower norms, we could call SuperBPE tokens bring more regularization, and consider this method a free lunch!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ggrigorev.me/tags/tokenization/">Tokenization</a></li>
      <li><a href="https://ggrigorev.me/tags/bpe/">Bpe</a></li>
      <li><a href="https://ggrigorev.me/tags/llm/">Llm</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ggrigorev.me/">George Grigorev Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
