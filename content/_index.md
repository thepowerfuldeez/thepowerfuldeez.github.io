+++
title = 'About'
date = '2025-09-07T00:00:00Z'
draft = false
+++

# Hi, I’m George

Currently: exploring pre‑training LLM training efficiency, sample efficiency, building small coding LLMs, and finance LLM agents.

Previously I worked at [Together AI](https://www.together.ai/) running distributed training on hundreds of GPUs and building evals. At [Snap](https://snap.com/) I developed multimodal LLMs that reached 100M+ users, did diffusion model pre‑training, built AI applications end‑to‑end, and contributed to research. I work across text, image, and speech.

## Highlights

- Large‑scale fine‑tuning and evaluation of open‑source LLMs (1B–405B), with a focus on training efficiency and model quality.
- Built distributed training platforms (60+ LLMs, 600+ GPUs) and LLM‑as‑a‑judge evaluation pipelines (Flyte‑orchestrated).
- Long‑context fine‑tuning: custom sequence parallelism with `flash_attn_varlen` for 131k‑context (LLaMA 3.1 70B) and 16k‑context (LLaMA 3.1 405B).

See also: [Publications](/publications/)
